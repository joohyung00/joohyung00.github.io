---
layout: base
---

{% include header.html type="page" %}

  <style>
    /* ===== Publications cards ===== */
    :root{ --pub-bg:#f6ebc9; } /* Î≥∏Î¨∏ Î∞∞Í≤Ω(#fcfbf9)Î≥¥Îã§ ÏÇ¥Ïßù Îçî ÏßÑÌïú ÎÖ∏ÎûÄÏÉâ */
    .pubs-wrap{ margin-top: 1.5rem; }
    details.pub-card{
      background: var(--pub-bg);
      border-radius: 16px;
      box-shadow: 0 6px 18px rgba(0,0,0,.08);
      overflow: hidden;
      margin-bottom: 14px;
      transition: box-shadow .2s ease, transform .2s ease;
    }
    details.pub-card:hover{ box-shadow: 0 10px 28px rgba(0,0,0,.12); }
    details.pub-card[open]{ transform: translateY(-1px); }

    /* header row (clickable) */
    details.pub-card > summary{
      list-style: none;
      cursor: pointer;
      padding: 16px;
      display: grid;
      grid-template-columns: 160px 1fr 28px; /* Ïç∏ÎÑ§Ïùº | Î≥∏Î¨∏ | ÌÜ†Í∏ÄÏïÑÏù¥ÏΩò */
      gap: 16px;
      align-items: center;
    }
    details.pub-card > summary::-webkit-details-marker{ display:none; }

    .pub-thumb{
      width: 100%;
      aspect-ratio: 4/3;
      border-radius: 10px;
      background: #e9deb6;   /* placeholder Î∞∞Í≤Ω */
      overflow: hidden;      /* Îë•Í∑º Î™®ÏÑúÎ¶¨ ÏïàÏ™ΩÏúºÎ°ú ÌÅ¥Î¶Ω */
    }
    .pub-thumb img{
      width: 100%;
      height: 100%;
      object-fit: cover;     /* Ï§ëÏïô ÌÅ¨Î°≠ */
      display: block;
    }

    .pub-badge{ margin-bottom: 6px; }
    .pub-title{ font-weight: 700; margin: 0; line-height: 1.35; }
    .pub-meta small{ display:block; opacity:.9; }
    .pub-emoji{ margin-right:.35rem; }

    .pub-toggle{
      font-size: 18px;
      opacity:.7;
      transition: transform .2s ease;
    }
    details.pub-card[open] .pub-toggle{ transform: rotate(180deg); }

    /* body (toggle area) */
    .pub-extra{
      border-top: 1px dashed rgba(0,0,0,.12);
      padding: 14px 16px 18px 16px;
      background: rgba(255,255,255,.35);
    }

    /* Î™®Î∞îÏùº */
    @media (max-width: 576px){
      details.pub-card > summary{
        grid-template-columns: 1fr; /* ÏÑ∏Î°ú Ïä§ÌÉù */
      }
      .pub-toggle{ justify-self: end; }
    }

    /* pub-extra: hero image + link pills */
    .pub-hero{
    display:block; max-width:820px; width:100%; height:auto;
    margin:0 auto 12px; border-radius:12px;
    box-shadow:0 4px 16px rgba(0,0,0,.08);
    }
    .pub-links{
    display:flex; justify-content:center; flex-wrap:wrap;
    gap:.5rem .75rem; margin-bottom:.25rem;
    }
    .pub-link{
    display:inline-flex; align-items:center; gap:.45rem;
    padding:.5rem .75rem; border-radius:999px;
    background:#fff7d6; border:1px solid rgba(0,0,0,.08);
    font-weight:600; color:#1c1c1c; text-decoration:none;
    }
    .pub-link:hover{ background:#ffeaa6; text-decoration:none; }
    .pub-link i{ font-size:1rem; line-height:1; }
  </style>

  <main class="container-md pubs-wrap">
    <div class="row">
      <div class="col-xl-10 offset-xl-1 col-lg-10 offset-lg-1">

        <!-- LILaC -->
        <details class="pub-card">
          <summary>
            <div class="pub-thumb"><img src="{{ '/assets/img/publications/lilac_thumb.jpeg' | relative_url }}" alt="LILaC preview"></div>
            <div class="pub-meta">
              <span class="badge badge-primary pub-badge">Conference</span>
              <h3 class="pub-title"><span class="pub-emoji" aria-hidden="true">üå∏</span>LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval</h3>
              <small><strong>Yun, J.</strong>, Lee, D., Han, W.</small>
              <small><em>EMNLP</em>, 2025.</small>
            </div>
            <div class="pub-toggle" aria-hidden="true">‚ñæ</div>
          </summary>
          <div class="pub-extra">
            <img class="pub-hero" src="{{ '/assets/img/publications/lilac.png' | relative_url }}" alt="LILaC figure">
            <div class="pub-links">
                <a class="pub-link" href="{{ 'https://aclanthology.org/2025.emnlp-main.1037/' | relative_url }}" target="_blank" rel="noopener" title="Paper (PDF)">
                <i class="fa-solid fa-file-pdf" aria-hidden="true"></i><span>Paper</span>
                </a>
                <a class="pub-link" href="{{ '/assets/slides/lilac.pptx' | relative_url }}" target="_blank" rel="noopener" title="Slides (PPTX)">
                <i class="fa-solid fa-file-powerpoint" aria-hidden="true"></i><span>Slides</span>
                </a>
                <a class="pub-link" href="https://github.com/joohyung00/lilac" target="_blank" rel="noopener" title="GitHub Repo">
                <i class="fa-brands fa-github" aria-hidden="true"></i><span>Code</span>
                </a>
            </div>
            <p><strong>Problem:</strong> Multimodal document retrieval, where data comes from text, tables, and images, requires both granular retrieval and the ability to reason across different documents. The key issue is balancing retrieval precision and enabling complex reasoning across these different modalities.</p>
            <p><strong>Existing Research Issues:</strong> Current methods like VisRAG (which treats everything as visual content) face problems with insufficient granularity and poor multihop reasoning. They often miss interdependencies between components or the relationships within documents.</p>
            <p><strong>Our Research:</strong> LILaC introduces a two-layered component graph for multimodal retrieval. The coarse layer supports efficient candidate generation, while the fine-grained layer facilitates detailed reasoning. Using a late-interaction strategy, LILaC dynamically traverses the graph to retrieve relevant subgraphs, improving both precision and recall in multihop scenarios. Extensive experiments show that LILaC outperforms existing systems in terms of both retrieval accuracy and reasoning.</p>
          </div>
        </details>

        <!-- HELIOS -->
        <details class="pub-card">
          <summary>
            <div class="pub-thumb"><img src="{{ '/assets/img/publications/helios_thumb.jpeg' | relative_url }}" alt="HELIOS preview"></div>
            <div class="pub-meta">
              <span class="badge badge-primary pub-badge">Conference</span>
              <h3 class="pub-title"><span class="pub-emoji" aria-hidden="true">‚òÄÔ∏è</span>HELIOS: Harmonizing Early Fusion, Late Fusion, and LLM Reasoning for Multi-Granular Table-Text Retrieval</h3>
              <small>Park, S., <strong>Yun, J.</strong>, Lee, J., Han, W.</small>
              <small><em>ACL</em>, 2025.</small>
            </div>
            <div class="pub-toggle" aria-hidden="true">‚ñæ</div>
          </summary>
          <div class="pub-extra">
            <img class="pub-hero" src="{{ '/assets/img/publications/helios.png' | relative_url }}" alt="HELIOS figure">
            <div class="pub-links">
                <a class="pub-link" href="https://aclanthology.org/2025.acl-long.1559.pdf" target="_blank" rel="noopener" title="Paper (PDF)">
                <i class="fa-solid fa-file-pdf" aria-hidden="true"></i><span>Paper</span>
                </a>
                <a class="pub-link" href="{{ '/assets/slides/helios.pdf' | relative_url }}" target="_blank" rel="noopener" title="Slides (PPTX)">
                <i class="fa-solid fa-file-powerpoint" aria-hidden="true"></i><span>Slides</span>
                </a>
                <a class="pub-link" href="https://github.com/pshlego/HELIOS" target="_blank" rel="noopener" title="GitHub Repo">
                <i class="fa-brands fa-github" aria-hidden="true"></i><span>Code</span>
                </a>
            </div>
            <p><strong>Problem:</strong> Multimodal document retrieval, especially when combining text, tables, and images, is challenging due to the need for effective reasoning across these different modalities and the issue of insufficient retrieval granularity.</p>
            <p><strong>Existing Research Issues:</strong> Prior methods like early fusion (which pre-aligns tables with passages) and late fusion (which aligns components dynamically) face limitations. Early fusion may retrieve irrelevant contexts, while late fusion risks missing important contexts, and both struggle with advanced reasoning tasks like multi-hop reasoning.</p>
            <p><strong>Our Research:</strong> We propose HELIOS, a framework that combines both early and late fusion techniques, followed by a reasoning step that uses LLMs to perform logical inference. By leveraging a layered graph structure, HELIOS achieves both high retrieval accuracy and efficient reasoning, outperforming existing systems across multiple benchmarks.</p>
          </div>
        </details>

        <!-- KDD Cup Report -->
        <details class="pub-card">
          <summary>
            <div class="pub-thumb"><img src="{{ '/assets/img/publications/kddcup_thumb.jpeg' | relative_url }}" alt="KDD Cup preview"></div>
            <div class="pub-meta">
              <span class="badge badge-info pub-badge">Workshop</span>
              <h3 class="pub-title"><span class="pub-emoji" aria-hidden="true">üèÜ</span>KDD Cup Meta CRAG 2024 Technical Report: Three-Step Question-Answering Framework</h3>
              <small>Park, S., Seok, J., Lee, J., <strong>Yun, J.</strong>, Lee, W.</small>
              <small><em>KDD Cup Workshop on Retrieval-Augmented Generation</em>, 2024.</small>
            </div>
            <div class="pub-toggle" aria-hidden="true">‚ñæ</div>
          </summary>
          <div class="pub-extra">
            <img class="pub-hero" src="{{ '/assets/img/publications/kddcup.png' | relative_url }}" alt="KDD Cup figure">
            <div class="pub-links">
                <a class="pub-link" href="https://openreview.net/pdf?id=G4Ei2QlKnv" target="_blank" rel="noopener" title="Paper (PDF)">
                <i class="fa-solid fa-file-pdf" aria-hidden="true"></i><span>Paper</span>
                </a>
                <a class="pub-link" href="https://github.com/joohyung00/kdd_cup_2024_crag" target="_blank" rel="noopener" title="GitHub Repo">
                <i class="fa-brands fa-github" aria-hidden="true"></i><span>Code</span>
                </a>
            </div>
            <p><strong>Problem:</strong> Large language models (LLMs) face challenges in question-answering tasks, particularly with hallucination‚Äîwhere answers deviate from real-world facts. The challenge is how to efficiently and accurately answer complex queries while minimizing hallucinations.</p>
            <p><strong>Existing Research Issues:</strong> Existing methods, such as Retrieval-Augmented Generation (RAG), attempt to improve LLMs by retrieving external knowledge, but often suffer from inefficiency, unnecessary retrievals, and propagation of incorrect information.</p>
            <p><strong>Our Research:</strong> We propose a three-step framework that improves the traditional RAG approach. First, it leverages LLM's parameterized knowledge to avoid unnecessary retrievals. Then, external sources are incorporated only when necessary. Finally, a verification step reassesses generated answers to ensure factual correctness, improving both accuracy and efficiency. Our approach has demonstrated significant improvements in multiple benchmarks.</p>
          </div>
        </details>

        <!-- ReCG -->
        <details class="pub-card">
          <summary>
            <div class="pub-thumb"><img src="{{ '/assets/img/publications/recg_thumb.jpeg' | relative_url }}" alt="KDD Cup preview"></div>
            <div class="pub-meta">
              <span class="badge badge-primary pub-badge">Conference</span>
              <h3 class="pub-title"><span class="pub-emoji" aria-hidden="true">üß©</span>ReCG: Bottom-Up JSON Schema Discovery Using a Repetitive Cluster-and-Generalize Framework</h3>
              <small><strong>Yun, J.</strong>, Tak, B., Han, W.</small>
              <small><em>PVLDB</em>, 2024.</small>
            </div>
            <div class="pub-toggle" aria-hidden="true">‚ñæ</div>
          </summary>
          <div class="pub-extra">
            <img class="pub-hero" src="{{ '/assets/img/publications/recg.png' | relative_url }}" alt="ReCG figure">
            <div class="pub-links">
                <a class="pub-link" href="https://dl.acm.org/doi/pdf/10.14778/3681954.3682019" target="_blank" rel="noopener" title="Paper (PDF)">
                <i class="fa-solid fa-file-pdf" aria-hidden="true"></i><span>Paper</span>
                </a>
                <a class="pub-link" href="{{ '/assets/slides/recg.pptx' | relative_url }}" target="_blank" rel="noopener" title="Slides (PPTX)">
                <i class="fa-solid fa-file-powerpoint" aria-hidden="true"></i><span>Slides</span>
                </a>
                <a class="pub-link" href="https://github.com/joohyung00/recg_vldb_2024" target="_blank" rel="noopener" title="GitHub Repo">
                <i class="fa-brands fa-github" aria-hidden="true"></i><span>Code</span>
                </a>
            </div>
            <p><strong>Problem:</strong> JSON documents are often schemaless, making it difficult to perform operations like querying, validation, and data management. Existing approaches for JSON schema discovery are top-down, which limits their ability to make accurate schema decisions for complex data.</p>
            <p><strong>Existing Research Issues:</strong> Top-down schema discovery methods struggle with incomplete visibility into child nodes, relying on heuristics that can lead to incorrect schema decisions. These methods also fail to handle heterogeneous data effectively.</p>
            <p><strong>Our Research:</strong> ReCG presents a bottom-up approach for discovering JSON schemas, starting from leaf nodes and generalizing upwards. This approach uses a clustering technique to identify schema patterns and minimizes schema complexity through the Minimum Description Length (MDL) principle. Our results show that ReCG significantly improves schema accuracy (up to 47%) and runs faster than state-of-the-art methods.</p>
          </div>
        </details>


      </div>
    </div>
  </main>
